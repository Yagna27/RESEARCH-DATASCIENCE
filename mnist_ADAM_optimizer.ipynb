{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mnist-- ADAM optimizer",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hea1431mAnT"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline \n",
        "import numpy as np\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vnmdCelm82t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a3ee5e2-beed-40d0-c25a-80165fa100d1"
      },
      "source": [
        "(x_train,y_train),(x_test,y_test) = keras.datasets.mnist.load_data()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbrxc_rxnXyL",
        "outputId": "5211997c-0cb7-4a1d-ff6d-0cb2ef079a4d"
      },
      "source": [
        "len(x_train)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60000"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VX9ZdOVxn0cc",
        "outputId": "7ec3cd73-10bc-4ddd-901f-05d6988acabd"
      },
      "source": [
        "len(x_test)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDi-Y71CpqZf",
        "outputId": "b367950e-cb9c-44e2-aee7-46e7a295898e"
      },
      "source": [
        "x_train[0].shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjZFJQIbxV-m",
        "outputId": "11438740-d63e-4a66-eb0f-60ed2413c245"
      },
      "source": [
        "x_train[0]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
              "         18,  18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170,\n",
              "        253, 253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253,\n",
              "        253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253,\n",
              "        253, 198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253,\n",
              "        205,  11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,\n",
              "         90,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253,\n",
              "        190,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190,\n",
              "        253,  70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
              "        241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,  45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,  46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39,\n",
              "        148, 229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221,\n",
              "        253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253,\n",
              "        253, 253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253,\n",
              "        195,  80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,\n",
              "         11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0]], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUdlPP_JnXKR"
      },
      "source": [
        "x_train=x_train/255\n",
        "x_test=x_test/255"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "id": "IvoyOvVI0vMe",
        "outputId": "b9f304c8-c879-4131-c08d-f004459e63a7"
      },
      "source": [
        "plt.matshow(x_train[11])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f8b670c6710>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 288x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQ_MMej41yk3",
        "outputId": "e2350eb7-3544-4506-e57a-d99759de35d2"
      },
      "source": [
        "y_train[:5]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 0, 4, 1, 9], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXVizb6h2OUZ",
        "outputId": "d22f2970-d264-47eb-bec6-24c23423c265"
      },
      "source": [
        "x_train.shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2ixsg3e2anm"
      },
      "source": [
        "x_train_flattened=x_train.reshape(len(x_train),28*28)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ATXwtME2kbs",
        "outputId": "3b9c9867-7b7c-4ec2-8411-6bb6d2e09928"
      },
      "source": [
        "x_train_flattened.shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 784)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1GYWviC26m1"
      },
      "source": [
        "x_test_flattened=x_test.reshape(len(x_test),28*28)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NI0YQm0z3Fjr",
        "outputId": "ea45c05a-3cee-4e2c-af10-188c93938b75"
      },
      "source": [
        "x_test_flattened.shape"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 784)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5GNmz6v3I9P",
        "outputId": "7a4a44a3-12af-49fa-f594-eee147e57b39"
      },
      "source": [
        "x_train_flattened[0]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.01176471, 0.07058824, 0.07058824,\n",
              "       0.07058824, 0.49411765, 0.53333333, 0.68627451, 0.10196078,\n",
              "       0.65098039, 1.        , 0.96862745, 0.49803922, 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.11764706, 0.14117647, 0.36862745, 0.60392157,\n",
              "       0.66666667, 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
              "       0.99215686, 0.88235294, 0.6745098 , 0.99215686, 0.94901961,\n",
              "       0.76470588, 0.25098039, 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.19215686, 0.93333333,\n",
              "       0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
              "       0.99215686, 0.99215686, 0.99215686, 0.98431373, 0.36470588,\n",
              "       0.32156863, 0.32156863, 0.21960784, 0.15294118, 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.07058824, 0.85882353, 0.99215686, 0.99215686,\n",
              "       0.99215686, 0.99215686, 0.99215686, 0.77647059, 0.71372549,\n",
              "       0.96862745, 0.94509804, 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.31372549, 0.61176471, 0.41960784, 0.99215686, 0.99215686,\n",
              "       0.80392157, 0.04313725, 0.        , 0.16862745, 0.60392157,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.05490196,\n",
              "       0.00392157, 0.60392157, 0.99215686, 0.35294118, 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.54509804,\n",
              "       0.99215686, 0.74509804, 0.00784314, 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.04313725, 0.74509804, 0.99215686,\n",
              "       0.2745098 , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.1372549 , 0.94509804, 0.88235294, 0.62745098,\n",
              "       0.42352941, 0.00392157, 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.31764706, 0.94117647, 0.99215686, 0.99215686, 0.46666667,\n",
              "       0.09803922, 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.17647059,\n",
              "       0.72941176, 0.99215686, 0.99215686, 0.58823529, 0.10588235,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.0627451 , 0.36470588,\n",
              "       0.98823529, 0.99215686, 0.73333333, 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.97647059, 0.99215686,\n",
              "       0.97647059, 0.25098039, 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.18039216, 0.50980392,\n",
              "       0.71764706, 0.99215686, 0.99215686, 0.81176471, 0.00784314,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.15294118,\n",
              "       0.58039216, 0.89803922, 0.99215686, 0.99215686, 0.99215686,\n",
              "       0.98039216, 0.71372549, 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.09411765, 0.44705882, 0.86666667, 0.99215686, 0.99215686,\n",
              "       0.99215686, 0.99215686, 0.78823529, 0.30588235, 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.09019608, 0.25882353, 0.83529412, 0.99215686,\n",
              "       0.99215686, 0.99215686, 0.99215686, 0.77647059, 0.31764706,\n",
              "       0.00784314, 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.07058824, 0.67058824, 0.85882353,\n",
              "       0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.76470588,\n",
              "       0.31372549, 0.03529412, 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.21568627, 0.6745098 ,\n",
              "       0.88627451, 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
              "       0.95686275, 0.52156863, 0.04313725, 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.53333333, 0.99215686, 0.99215686, 0.99215686,\n",
              "       0.83137255, 0.52941176, 0.51764706, 0.0627451 , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        ])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIdv-YEy_Bbk"
      },
      "source": [
        "dense single layer with ADAM\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idFw_VCB3eRY"
      },
      "source": [
        "modelA1=keras.Sequential([\n",
        "             keras.layers.Dense(10,input_shape=(784,),activation='sigmoid')     \n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYjJXiw44Ye8"
      },
      "source": [
        "modelA1.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        "\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Ge7CD905loy",
        "outputId": "65b66323-2eb2-499f-9635-4119ec24786b"
      },
      "source": [
        "modelA1.fit(x_train_flattened,y_train,epochs=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 9.3326 - accuracy: 0.8423\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 3s 1ms/step - loss: 6.0202 - accuracy: 0.8783\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 3s 1ms/step - loss: 5.7024 - accuracy: 0.8816\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 3s 1ms/step - loss: 5.5525 - accuracy: 0.8860\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 5.3124 - accuracy: 0.8873\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0a07f222d0>"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sjDNxzLLSBv"
      },
      "source": [
        "dense two-layer with ADAM\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oORp-_Gp6AyD"
      },
      "source": [
        "modelA2=keras.Sequential([\n",
        "             keras.layers.Dense(10,input_shape=(784,),activation='sigmoid') , \n",
        "             keras.layers.Dense(10,input_shape=(784,),activation='sigmoid')  \n",
        "                \n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWbz9hFPGdWB"
      },
      "source": [
        "modelA2.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        "\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1F6nBCIHGmiG",
        "outputId": "9b0f3cac-e798-4419-cf42-f09530a0e15d"
      },
      "source": [
        "modelA2.fit(x_train_flattened,y_train,epochs=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.2600 - accuracy: 0.6565\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.6775 - accuracy: 0.8114\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.5983 - accuracy: 0.8248\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.5383 - accuracy: 0.8427\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.5329 - accuracy: 0.8441\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0a08675b50>"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33Q7kegjLtap"
      },
      "source": [
        "dense 3-layer with ADAM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xcWfkEZ9Gq-E",
        "outputId": "aacce668-d0b8-48af-a1f0-3f8c806a5c94"
      },
      "source": [
        "modelA3=keras.Sequential([\n",
        "             keras.layers.Dense(10,input_shape=(784,),activation='sigmoid') , \n",
        "             keras.layers.Dense(10,input_shape=(784,),activation='sigmoid')  ,\n",
        "             keras.layers.Dense(10,input_shape=(784,),activation='sigmoid')  \n",
        "                \n",
        "])\n",
        "modelA3.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        "\n",
        ")\n",
        "modelA3.fit(x_train_flattened,y_train,epochs=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.6979 - accuracy: 0.4878\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.1066 - accuracy: 0.6492\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.8888 - accuracy: 0.7049\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.7778 - accuracy: 0.7474\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.7223 - accuracy: 0.7746\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0a0840b9d0>"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTzMEBfMMFdo"
      },
      "source": [
        "dense 4-layer with ADAM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zWZyFetL4ly",
        "outputId": "5dd9275d-1121-476e-e5a7-d73cc194b3b8"
      },
      "source": [
        "modelA4=keras.Sequential([\n",
        "             keras.layers.Dense(10,input_shape=(784,),activation='sigmoid') , \n",
        "             keras.layers.Dense(10,input_shape=(784,),activation='sigmoid')  ,\n",
        "             keras.layers.Dense(10,input_shape=(784,),activation='sigmoid')  ,\n",
        "             keras.layers.Dense(10,input_shape=(784,),activation='sigmoid')  \n",
        "                \n",
        "])\n",
        "modelA4.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        "\n",
        ")\n",
        "modelA4.fit(x_train_flattened,y_train,epochs=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.7748 - accuracy: 0.4165\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.1262 - accuracy: 0.6355\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.9117 - accuracy: 0.7199\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.7457 - accuracy: 0.7880\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.6692 - accuracy: 0.8061\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0a081e41d0>"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUC51IIPNFsQ"
      },
      "source": [
        "dense 5-layer with ADAM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxemNX9RMXUB",
        "outputId": "d320d5e6-7474-4281-8884-293a1c40f1f2"
      },
      "source": [
        "modelA5=keras.Sequential([\n",
        "             keras.layers.Dense(10,input_shape=(784,),activation='sigmoid') , \n",
        "             keras.layers.Dense(10,input_shape=(784,),activation='sigmoid')  ,\n",
        "             keras.layers.Dense(10,input_shape=(784,),activation='sigmoid')  ,\n",
        "             keras.layers.Dense(10,input_shape=(784,),activation='sigmoid')  ,\n",
        "             keras.layers.Dense(10,input_shape=(784,),activation='sigmoid')\n",
        "                \n",
        "])\n",
        "modelA5.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        "\n",
        ")\n",
        "modelA5.fit(x_train_flattened,y_train,epochs=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 2.0406 - accuracy: 0.2428\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.5085 - accuracy: 0.4504\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.2869 - accuracy: 0.5163\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.1559 - accuracy: 0.5599\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.0630 - accuracy: 0.6327\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0a07f6d5d0>"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiM7pJ3NNT3b"
      },
      "source": [
        "ADAM1ACC=[0.8873,0.8441,0.7746,0.8061,0.6327] #accuracy of above five models keeping epochs and parameters constant "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRd857AmOETI",
        "outputId": "ef484015-2adf-4846-bc72-452a44c0555f"
      },
      "source": [
        "modelA11=keras.Sequential([\n",
        "             keras.layers.Dense(10,input_shape=(784,),activation='sigmoid') , \n",
        "             keras.layers.Dense(100,input_shape=(784,),activation='sigmoid')\n",
        "                \n",
        "])\n",
        "modelA11.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        "\n",
        ")\n",
        "modelA11.fit(x_train_flattened,y_train,epochs=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.7179 - accuracy: 0.3382\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.3970 - accuracy: 0.5013\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.1651 - accuracy: 0.6394\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.0400 - accuracy: 0.6743\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.9572 - accuracy: 0.7146\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0a07d42a90>"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6VvQgGgrOTIm",
        "outputId": "154a5ca9-6bd0-43bd-e669-b0482d6722be"
      },
      "source": [
        "modelA12=keras.Sequential([\n",
        "             keras.layers.Dense(10,input_shape=(784,),activation='sigmoid') , \n",
        "             keras.layers.Dense(100,input_shape=(784,),activation='sigmoid')  ,\n",
        "             keras.layers.Dense(1000,input_shape=(784,),activation='sigmoid')  \n",
        "                \n",
        "])\n",
        "modelA12.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        "\n",
        ")\n",
        "modelA12.fit(x_train_flattened,y_train,epochs=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 1.7550 - accuracy: 0.4165\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.8760 - accuracy: 0.7084\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.6854 - accuracy: 0.7840\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.5985 - accuracy: 0.8206\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.5749 - accuracy: 0.8298\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0a02ed4690>"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YuSo4NrWO_4z",
        "outputId": "21c56f1b-ac1b-4805-a02d-1a5e2889ca76"
      },
      "source": [
        "modelA13=keras.Sequential([\n",
        "             keras.layers.Dense(10,input_shape=(784,),activation='sigmoid') , \n",
        "             keras.layers.Dense(100,input_shape=(784,),activation='sigmoid')  ,\n",
        "             keras.layers.Dense(1000,input_shape=(784,),activation='sigmoid')  ,\n",
        "             keras.layers.Dense(1000,input_shape=(784,),activation='sigmoid')  \n",
        "                \n",
        "])\n",
        "modelA13.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        "\n",
        ")\n",
        "modelA13.fit(x_train_flattened,y_train,epochs=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 28s 15ms/step - loss: 0.8767 - accuracy: 0.7124\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 28s 15ms/step - loss: 0.3467 - accuracy: 0.8985\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 28s 15ms/step - loss: 0.2777 - accuracy: 0.9181\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 28s 15ms/step - loss: 0.2477 - accuracy: 0.9264\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 28s 15ms/step - loss: 0.2272 - accuracy: 0.9321\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f62b0a21290>"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0V2qjs-Pau2"
      },
      "source": [
        "ADAM2ACC=[0.8873,0.7146, 0.8298,0.8177] #accuracy of above four models keeping epochs  constant  and increases parameters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_LcywFKk84_R",
        "outputId": "090a02c2-5d94-4ae4-a3bd-f76f07b88ade"
      },
      "source": [
        "modelA132=keras.Sequential([\n",
        "             keras.layers.Dense(10,input_shape=(784,),activation='sigmoid') , \n",
        "             keras.layers.Dense(100,input_shape=(784,),activation='sigmoid')  ,\n",
        "             keras.layers.Dense(1000,input_shape=(784,),activation='sigmoid')  ,\n",
        "             keras.layers.Dense(1000,input_shape=(784,),activation='sigmoid')  \n",
        "                \n",
        "])\n",
        "modelA132.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        "\n",
        ")\n",
        "modelA132.fit(x_train_flattened,y_train,epochs=20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "1875/1875 [==============================] - 27s 14ms/step - loss: 0.9671 - accuracy: 0.6597\n",
            "Epoch 2/20\n",
            "1875/1875 [==============================] - 27s 14ms/step - loss: 0.3877 - accuracy: 0.8885\n",
            "Epoch 3/20\n",
            "1875/1875 [==============================] - 27s 14ms/step - loss: 0.3039 - accuracy: 0.9136\n",
            "Epoch 4/20\n",
            "1875/1875 [==============================] - 27s 14ms/step - loss: 0.2656 - accuracy: 0.9218\n",
            "Epoch 5/20\n",
            "1875/1875 [==============================] - 27s 14ms/step - loss: 0.2424 - accuracy: 0.9281\n",
            "Epoch 6/20\n",
            "1875/1875 [==============================] - 27s 14ms/step - loss: 0.2269 - accuracy: 0.9312\n",
            "Epoch 7/20\n",
            "1875/1875 [==============================] - 27s 14ms/step - loss: 0.2142 - accuracy: 0.9352\n",
            "Epoch 8/20\n",
            "1875/1875 [==============================] - 27s 14ms/step - loss: 0.2050 - accuracy: 0.9382\n",
            "Epoch 9/20\n",
            "1875/1875 [==============================] - 27s 15ms/step - loss: 0.1971 - accuracy: 0.9405\n",
            "Epoch 10/20\n",
            "1875/1875 [==============================] - 27s 14ms/step - loss: 0.1902 - accuracy: 0.9420\n",
            "Epoch 11/20\n",
            "1875/1875 [==============================] - 27s 14ms/step - loss: 0.1842 - accuracy: 0.9439\n",
            "Epoch 12/20\n",
            "1875/1875 [==============================] - 27s 15ms/step - loss: 0.1792 - accuracy: 0.9453\n",
            "Epoch 13/20\n",
            "1875/1875 [==============================] - 27s 15ms/step - loss: 0.1743 - accuracy: 0.9474\n",
            "Epoch 14/20\n",
            "1875/1875 [==============================] - 28s 15ms/step - loss: 0.1695 - accuracy: 0.9480\n",
            "Epoch 15/20\n",
            "1875/1875 [==============================] - 27s 15ms/step - loss: 0.1661 - accuracy: 0.9489\n",
            "Epoch 16/20\n",
            "1875/1875 [==============================] - 27s 14ms/step - loss: 0.1614 - accuracy: 0.9506\n",
            "Epoch 17/20\n",
            "1875/1875 [==============================] - 27s 14ms/step - loss: 0.1575 - accuracy: 0.9510\n",
            "Epoch 18/20\n",
            "1875/1875 [==============================] - 27s 15ms/step - loss: 0.1546 - accuracy: 0.9525\n",
            "Epoch 19/20\n",
            "1875/1875 [==============================] - 27s 14ms/step - loss: 0.1516 - accuracy: 0.9526\n",
            "Epoch 20/20\n",
            "1875/1875 [==============================] - 27s 15ms/step - loss: 0.1495 - accuracy: 0.9533\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f62b05cd850>"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-aEeanZQRj3",
        "outputId": "2d6194a4-c349-4c00-e8cf-2e23adcd69b1"
      },
      "source": [
        "modelA133=keras.Sequential([\n",
        "             keras.layers.Dense(10,input_shape=(784,),activation='sigmoid') , \n",
        "             keras.layers.Dense(100,input_shape=(784,),activation='sigmoid')  ,\n",
        "             keras.layers.Dense(1000,input_shape=(784,),activation='sigmoid')  ,\n",
        "             keras.layers.Dense(1000,input_shape=(784,),activation='sigmoid')  \n",
        "                \n",
        "])\n",
        "modelA133.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        "\n",
        ")\n",
        "modelA133.fit(x_train_flattened,y_train,epochs=70)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/70\n",
            "1875/1875 [==============================] - 28s 15ms/step - loss: 0.8487 - accuracy: 0.7258\n",
            "Epoch 2/70\n",
            "1875/1875 [==============================] - 27s 14ms/step - loss: 0.3676 - accuracy: 0.8931\n",
            "Epoch 3/70\n",
            "1875/1875 [==============================] - 27s 15ms/step - loss: 0.3093 - accuracy: 0.9082\n",
            "Epoch 4/70\n",
            "1875/1875 [==============================] - 27s 14ms/step - loss: 0.2764 - accuracy: 0.9169\n",
            "Epoch 5/70\n",
            "1875/1875 [==============================] - 27s 14ms/step - loss: 0.2543 - accuracy: 0.9229\n",
            "Epoch 6/70\n",
            "1875/1875 [==============================] - 27s 14ms/step - loss: 0.2355 - accuracy: 0.9276\n",
            "Epoch 7/70\n",
            "1875/1875 [==============================] - 27s 14ms/step - loss: 0.2233 - accuracy: 0.9312\n",
            "Epoch 8/70\n",
            "1875/1875 [==============================] - 27s 14ms/step - loss: 0.2115 - accuracy: 0.9351\n",
            "Epoch 9/70\n",
            "1875/1875 [==============================] - 27s 14ms/step - loss: 0.2032 - accuracy: 0.9372\n",
            "Epoch 10/70\n",
            "1875/1875 [==============================] - 27s 15ms/step - loss: 0.1951 - accuracy: 0.9398\n",
            "Epoch 11/70\n",
            "1875/1875 [==============================] - 27s 15ms/step - loss: 0.1891 - accuracy: 0.9408\n",
            "Epoch 12/70\n",
            "1875/1875 [==============================] - 27s 15ms/step - loss: 0.1835 - accuracy: 0.9426\n",
            "Epoch 13/70\n",
            "1875/1875 [==============================] - 27s 15ms/step - loss: 0.1782 - accuracy: 0.9444\n",
            "Epoch 14/70\n",
            "1875/1875 [==============================] - 27s 15ms/step - loss: 0.1751 - accuracy: 0.9450\n",
            "Epoch 15/70\n",
            "1875/1875 [==============================] - 27s 15ms/step - loss: 0.1707 - accuracy: 0.9463\n",
            "Epoch 16/70\n",
            "1875/1875 [==============================] - 27s 15ms/step - loss: 0.1673 - accuracy: 0.9472\n",
            "Epoch 17/70\n",
            "1875/1875 [==============================] - 27s 15ms/step - loss: 0.1630 - accuracy: 0.9485\n",
            "Epoch 18/70\n",
            "1875/1875 [==============================] - 27s 15ms/step - loss: 0.1586 - accuracy: 0.9505\n",
            "Epoch 19/70\n",
            "1875/1875 [==============================] - 27s 14ms/step - loss: 0.1566 - accuracy: 0.9508\n",
            "Epoch 20/70\n",
            "1875/1875 [==============================] - 27s 15ms/step - loss: 0.1543 - accuracy: 0.9507\n",
            "Epoch 21/70\n",
            "1875/1875 [==============================] - 27s 15ms/step - loss: 0.1509 - accuracy: 0.9514\n",
            "Epoch 22/70\n",
            "1875/1875 [==============================] - 27s 15ms/step - loss: 0.1479 - accuracy: 0.9528\n",
            "Epoch 23/70\n",
            "1875/1875 [==============================] - 27s 15ms/step - loss: 0.1456 - accuracy: 0.9538\n",
            "Epoch 24/70\n",
            "1875/1875 [==============================] - 27s 15ms/step - loss: 0.1429 - accuracy: 0.9548\n",
            "Epoch 25/70\n",
            "1875/1875 [==============================] - 27s 15ms/step - loss: 0.1404 - accuracy: 0.9550\n",
            "Epoch 26/70\n",
            "1875/1875 [==============================] - 27s 14ms/step - loss: 0.1377 - accuracy: 0.9558\n",
            "Epoch 27/70\n",
            "1875/1875 [==============================] - 27s 14ms/step - loss: 0.1356 - accuracy: 0.9569\n",
            "Epoch 28/70\n",
            "1875/1875 [==============================] - 27s 14ms/step - loss: 0.1327 - accuracy: 0.9574\n",
            "Epoch 29/70\n",
            "1875/1875 [==============================] - 27s 14ms/step - loss: 0.1299 - accuracy: 0.9579\n",
            "Epoch 30/70\n",
            "1875/1875 [==============================] - 27s 14ms/step - loss: 0.1289 - accuracy: 0.9579\n",
            "Epoch 31/70\n",
            "1875/1875 [==============================] - 27s 15ms/step - loss: 0.1264 - accuracy: 0.9591\n",
            "Epoch 32/70\n",
            "1875/1875 [==============================] - 27s 15ms/step - loss: 0.1245 - accuracy: 0.9595\n",
            "Epoch 33/70\n",
            "1875/1875 [==============================] - 27s 15ms/step - loss: 0.1225 - accuracy: 0.9602\n",
            "Epoch 34/70\n",
            "1875/1875 [==============================] - 27s 15ms/step - loss: 0.1217 - accuracy: 0.9606\n",
            "Epoch 35/70\n",
            "1875/1875 [==============================] - 27s 15ms/step - loss: 0.1189 - accuracy: 0.9617\n",
            "Epoch 36/70\n",
            "1875/1875 [==============================] - 27s 15ms/step - loss: 0.1166 - accuracy: 0.9621\n",
            "Epoch 37/70\n",
            "1875/1875 [==============================] - 27s 15ms/step - loss: 0.1159 - accuracy: 0.9625\n",
            "Epoch 38/70\n",
            "1875/1875 [==============================] - 27s 15ms/step - loss: 0.1138 - accuracy: 0.9627\n",
            "Epoch 39/70\n",
            "1875/1875 [==============================] - 27s 15ms/step - loss: 0.1142 - accuracy: 0.9627\n",
            "Epoch 40/70\n",
            "1875/1875 [==============================] - 28s 15ms/step - loss: 0.1096 - accuracy: 0.9642\n",
            "Epoch 41/70\n",
            "1875/1875 [==============================] - 27s 15ms/step - loss: 0.1085 - accuracy: 0.9643\n",
            "Epoch 42/70\n",
            "1875/1875 [==============================] - 27s 15ms/step - loss: 0.1078 - accuracy: 0.9645\n",
            "Epoch 43/70\n",
            "1875/1875 [==============================] - 27s 14ms/step - loss: 0.1070 - accuracy: 0.9647\n",
            "Epoch 44/70\n",
            "1875/1875 [==============================] - 28s 15ms/step - loss: 0.1044 - accuracy: 0.9654\n",
            "Epoch 45/70\n",
            "1875/1875 [==============================] - 28s 15ms/step - loss: 0.1039 - accuracy: 0.9659\n",
            "Epoch 46/70\n",
            "1875/1875 [==============================] - 27s 15ms/step - loss: 0.1024 - accuracy: 0.9667\n",
            "Epoch 47/70\n",
            "1875/1875 [==============================] - 27s 15ms/step - loss: 0.1006 - accuracy: 0.9672\n",
            "Epoch 48/70\n",
            "1875/1875 [==============================] - 27s 14ms/step - loss: 0.0998 - accuracy: 0.9663\n",
            "Epoch 49/70\n",
            "1875/1875 [==============================] - 27s 15ms/step - loss: 0.0980 - accuracy: 0.9672\n",
            "Epoch 50/70\n",
            "1875/1875 [==============================] - 27s 14ms/step - loss: 0.0974 - accuracy: 0.9673\n",
            "Epoch 51/70\n",
            "1875/1875 [==============================] - 27s 14ms/step - loss: 0.0949 - accuracy: 0.9684\n",
            "Epoch 52/70\n",
            "1875/1875 [==============================] - 27s 14ms/step - loss: 0.0948 - accuracy: 0.9681\n",
            "Epoch 53/70\n",
            "1875/1875 [==============================] - 27s 14ms/step - loss: 0.0944 - accuracy: 0.9695\n",
            "Epoch 54/70\n",
            "1875/1875 [==============================] - 27s 15ms/step - loss: 0.0914 - accuracy: 0.9696\n",
            "Epoch 55/70\n",
            "1875/1875 [==============================] - 27s 14ms/step - loss: 0.0902 - accuracy: 0.9701\n",
            "Epoch 56/70\n",
            "1875/1875 [==============================] - 27s 15ms/step - loss: 0.0895 - accuracy: 0.9697\n",
            "Epoch 57/70\n",
            "1875/1875 [==============================] - 27s 15ms/step - loss: 0.0879 - accuracy: 0.9706\n",
            "Epoch 58/70\n",
            "1875/1875 [==============================] - 27s 14ms/step - loss: 0.0873 - accuracy: 0.9703\n",
            "Epoch 59/70\n",
            "1875/1875 [==============================] - 27s 15ms/step - loss: 0.0862 - accuracy: 0.9711\n",
            "Epoch 60/70\n",
            "1875/1875 [==============================] - 27s 15ms/step - loss: 0.0855 - accuracy: 0.9715\n",
            "Epoch 61/70\n",
            "1875/1875 [==============================] - 27s 15ms/step - loss: 0.0834 - accuracy: 0.9725\n",
            "Epoch 62/70\n",
            "1875/1875 [==============================] - 27s 15ms/step - loss: 0.0831 - accuracy: 0.9717\n",
            "Epoch 63/70\n",
            "1875/1875 [==============================] - 27s 15ms/step - loss: 0.0816 - accuracy: 0.9729\n",
            "Epoch 64/70\n",
            "1875/1875 [==============================] - 27s 14ms/step - loss: 0.0815 - accuracy: 0.9724\n",
            "Epoch 65/70\n",
            "1875/1875 [==============================] - 27s 14ms/step - loss: 0.0788 - accuracy: 0.9735\n",
            "Epoch 66/70\n",
            "1875/1875 [==============================] - 27s 15ms/step - loss: 0.0789 - accuracy: 0.9731\n",
            "Epoch 67/70\n",
            "1875/1875 [==============================] - 27s 15ms/step - loss: 0.0784 - accuracy: 0.9733\n",
            "Epoch 68/70\n",
            "1875/1875 [==============================] - 27s 15ms/step - loss: 0.0769 - accuracy: 0.9739\n",
            "Epoch 69/70\n",
            "1875/1875 [==============================] - 28s 15ms/step - loss: 0.0759 - accuracy: 0.9747\n",
            "Epoch 70/70\n",
            "1875/1875 [==============================] - 27s 15ms/step - loss: 0.0754 - accuracy: 0.9744\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f62b04fb710>"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "h3zDnTERQyGu",
        "outputId": "1886dd24-2089-43cf-8feb-734b27ac33a4"
      },
      "source": [
        "modelA131=keras.Sequential([\n",
        "             keras.layers.Dense(10,input_shape=(784,),activation='sigmoid') , \n",
        "             keras.layers.Dense(100,input_shape=(784,),activation='sigmoid')  ,\n",
        "             keras.layers.Dense(1000,input_shape=(784,),activation='sigmoid')  ,\n",
        "             keras.layers.Dense(1000,input_shape=(784,),activation='sigmoid')  \n",
        "                \n",
        "])\n",
        "modelA131.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        "\n",
        ")\n",
        "modelA131.fit(x_train_flattened,y_train,epochs=50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "1875/1875 [==============================] - 27s 14ms/step - loss: 0.8879 - accuracy: 0.6981\n",
            "Epoch 2/50\n",
            "1875/1875 [==============================] - 26s 14ms/step - loss: 0.3636 - accuracy: 0.8949\n",
            "Epoch 3/50\n",
            "1875/1875 [==============================] - 27s 14ms/step - loss: 0.2856 - accuracy: 0.9171\n",
            "Epoch 4/50\n",
            "1875/1875 [==============================] - 27s 14ms/step - loss: 0.2503 - accuracy: 0.9265\n",
            "Epoch 5/50\n",
            "1875/1875 [==============================] - 27s 14ms/step - loss: 0.2295 - accuracy: 0.9310\n",
            "Epoch 6/50\n",
            "1875/1875 [==============================] - 27s 15ms/step - loss: 0.2122 - accuracy: 0.9357\n",
            "Epoch 7/50\n",
            "1875/1875 [==============================] - 27s 14ms/step - loss: 0.2022 - accuracy: 0.9385\n",
            "Epoch 8/50\n",
            "1875/1875 [==============================] - 27s 15ms/step - loss: 0.1932 - accuracy: 0.9408\n",
            "Epoch 9/50\n",
            "1875/1875 [==============================] - 27s 14ms/step - loss: 0.1858 - accuracy: 0.9433\n",
            "Epoch 10/50\n",
            "1875/1875 [==============================] - 27s 14ms/step - loss: 0.1806 - accuracy: 0.9449\n",
            "Epoch 11/50\n",
            "1875/1875 [==============================] - 27s 15ms/step - loss: 0.1751 - accuracy: 0.9460\n",
            "Epoch 12/50\n",
            "1875/1875 [==============================] - 27s 15ms/step - loss: 0.1708 - accuracy: 0.9470\n",
            "Epoch 13/50\n",
            "1875/1875 [==============================] - 27s 14ms/step - loss: 0.1659 - accuracy: 0.9484\n",
            "Epoch 14/50\n",
            "1875/1875 [==============================] - 27s 14ms/step - loss: 0.1618 - accuracy: 0.9497\n",
            "Epoch 15/50\n",
            "1875/1875 [==============================] - 27s 14ms/step - loss: 0.1588 - accuracy: 0.9502\n",
            "Epoch 16/50\n",
            "1875/1875 [==============================] - 27s 14ms/step - loss: 0.1563 - accuracy: 0.9513\n",
            "Epoch 17/50\n",
            "1875/1875 [==============================] - 28s 15ms/step - loss: 0.1532 - accuracy: 0.9518\n",
            "Epoch 18/50\n",
            "1875/1875 [==============================] - 27s 14ms/step - loss: 0.1497 - accuracy: 0.9533\n",
            "Epoch 19/50\n",
            "1875/1875 [==============================] - 27s 15ms/step - loss: 0.1467 - accuracy: 0.9537\n",
            "Epoch 20/50\n",
            "1875/1875 [==============================] - 27s 15ms/step - loss: 0.1449 - accuracy: 0.9543\n",
            "Epoch 21/50\n",
            "1875/1875 [==============================] - 27s 14ms/step - loss: 0.1420 - accuracy: 0.9556\n",
            "Epoch 22/50\n",
            "1875/1875 [==============================] - 27s 14ms/step - loss: 0.1398 - accuracy: 0.9562\n",
            "Epoch 23/50\n",
            "1875/1875 [==============================] - 28s 15ms/step - loss: 0.1368 - accuracy: 0.9571\n",
            "Epoch 24/50\n",
            "1875/1875 [==============================] - 27s 14ms/step - loss: 0.1352 - accuracy: 0.9568\n",
            "Epoch 25/50\n",
            "1875/1875 [==============================] - 27s 15ms/step - loss: 0.1341 - accuracy: 0.9579\n",
            "Epoch 26/50\n",
            "1875/1875 [==============================] - 27s 15ms/step - loss: 0.1316 - accuracy: 0.9586\n",
            "Epoch 27/50\n",
            "1875/1875 [==============================] - 27s 15ms/step - loss: 0.1295 - accuracy: 0.9590\n",
            "Epoch 28/50\n",
            "1875/1875 [==============================] - 27s 15ms/step - loss: 0.1278 - accuracy: 0.9594\n",
            "Epoch 29/50\n",
            "1875/1875 [==============================] - 27s 15ms/step - loss: 0.1263 - accuracy: 0.9590\n",
            "Epoch 30/50\n",
            "1875/1875 [==============================] - 27s 15ms/step - loss: 0.1233 - accuracy: 0.9606\n",
            "Epoch 31/50\n",
            "1875/1875 [==============================] - 27s 15ms/step - loss: 0.1228 - accuracy: 0.9606\n",
            "Epoch 32/50\n",
            "1875/1875 [==============================] - 27s 15ms/step - loss: 0.1210 - accuracy: 0.9612\n",
            "Epoch 33/50\n",
            "1875/1875 [==============================] - 27s 15ms/step - loss: 0.1198 - accuracy: 0.9621\n",
            "Epoch 34/50\n",
            "1875/1875 [==============================] - 27s 15ms/step - loss: 0.1184 - accuracy: 0.9622\n",
            "Epoch 35/50\n",
            "1875/1875 [==============================] - 28s 15ms/step - loss: 0.1167 - accuracy: 0.9628\n",
            "Epoch 36/50\n",
            "1875/1875 [==============================] - 27s 15ms/step - loss: 0.1151 - accuracy: 0.9627\n",
            "Epoch 37/50\n",
            "1875/1875 [==============================] - 27s 15ms/step - loss: 0.1128 - accuracy: 0.9644\n",
            "Epoch 38/50\n",
            "1875/1875 [==============================] - 27s 15ms/step - loss: 0.1124 - accuracy: 0.9636\n",
            "Epoch 39/50\n",
            "1875/1875 [==============================] - 27s 15ms/step - loss: 0.1106 - accuracy: 0.9643\n",
            "Epoch 40/50\n",
            "1875/1875 [==============================] - 28s 15ms/step - loss: 0.1101 - accuracy: 0.9640\n",
            "Epoch 41/50\n",
            "1875/1875 [==============================] - 27s 15ms/step - loss: 0.1074 - accuracy: 0.9650\n",
            "Epoch 42/50\n",
            "1875/1875 [==============================] - 27s 14ms/step - loss: 0.1074 - accuracy: 0.9645\n",
            "Epoch 43/50\n",
            "1875/1875 [==============================] - 27s 14ms/step - loss: 0.1045 - accuracy: 0.9662\n",
            "Epoch 44/50\n",
            "1875/1875 [==============================] - 27s 14ms/step - loss: 0.1046 - accuracy: 0.9664\n",
            "Epoch 45/50\n",
            "1875/1875 [==============================] - 27s 14ms/step - loss: 0.1027 - accuracy: 0.9659\n",
            "Epoch 46/50\n",
            "1875/1875 [==============================] - 27s 14ms/step - loss: 0.1018 - accuracy: 0.9667\n",
            "Epoch 47/50\n",
            "1875/1875 [==============================] - 27s 14ms/step - loss: 0.1004 - accuracy: 0.9667\n",
            "Epoch 48/50\n",
            "1875/1875 [==============================] - 27s 14ms/step - loss: 0.0994 - accuracy: 0.9670\n",
            "Epoch 49/50\n",
            "1875/1875 [==============================] - 27s 14ms/step - loss: 0.0979 - accuracy: 0.9677\n",
            "Epoch 50/50\n",
            "1875/1875 [==============================] - 27s 15ms/step - loss: 0.0963 - accuracy: 0.9681\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f62a2da98d0>"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZVZ7gBLQ1wS",
        "outputId": "dcc6bd16-13a1-42be-cd23-b40eda2e47f1"
      },
      "source": [
        "modelA134=keras.Sequential([\n",
        "             keras.layers.Dense(10,input_shape=(784,),activation='sigmoid') , \n",
        "             keras.layers.Dense(100,input_shape=(784,),activation='sigmoid')  ,\n",
        "             keras.layers.Dense(1000,input_shape=(784,),activation='sigmoid')  ,\n",
        "             keras.layers.Dense(1000,input_shape=(784,),activation='sigmoid')  \n",
        "                \n",
        "])\n",
        "modelA134.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        "\n",
        ")\n",
        "modelA134.fit(x_train_flattened,y_train,epochs=100)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1875/1875 [==============================] - 10s 4ms/step - loss: 0.8470 - accuracy: 0.7215\n",
            "Epoch 2/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.3565 - accuracy: 0.8978\n",
            "Epoch 3/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.2992 - accuracy: 0.9133\n",
            "Epoch 4/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.2675 - accuracy: 0.9214\n",
            "Epoch 5/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.2456 - accuracy: 0.9267\n",
            "Epoch 6/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.2288 - accuracy: 0.9312\n",
            "Epoch 7/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.2168 - accuracy: 0.9344\n",
            "Epoch 8/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.2085 - accuracy: 0.9372\n",
            "Epoch 9/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.2004 - accuracy: 0.9394\n",
            "Epoch 10/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1929 - accuracy: 0.9409\n",
            "Epoch 11/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1871 - accuracy: 0.9430\n",
            "Epoch 12/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1818 - accuracy: 0.9445\n",
            "Epoch 13/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1772 - accuracy: 0.9461\n",
            "Epoch 14/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1731 - accuracy: 0.9463\n",
            "Epoch 15/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1704 - accuracy: 0.9476\n",
            "Epoch 16/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1667 - accuracy: 0.9480\n",
            "Epoch 17/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1635 - accuracy: 0.9496\n",
            "Epoch 18/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1609 - accuracy: 0.9500\n",
            "Epoch 19/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1574 - accuracy: 0.9503\n",
            "Epoch 20/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1551 - accuracy: 0.9520\n",
            "Epoch 21/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1528 - accuracy: 0.9523\n",
            "Epoch 22/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1505 - accuracy: 0.9534\n",
            "Epoch 23/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1488 - accuracy: 0.9538\n",
            "Epoch 24/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1460 - accuracy: 0.9535\n",
            "Epoch 25/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1440 - accuracy: 0.9548\n",
            "Epoch 26/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1424 - accuracy: 0.9552\n",
            "Epoch 27/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1401 - accuracy: 0.9554\n",
            "Epoch 28/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1385 - accuracy: 0.9566\n",
            "Epoch 29/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1369 - accuracy: 0.9561\n",
            "Epoch 30/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1343 - accuracy: 0.9574\n",
            "Epoch 31/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1338 - accuracy: 0.9579\n",
            "Epoch 32/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1319 - accuracy: 0.9583\n",
            "Epoch 33/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1291 - accuracy: 0.9590\n",
            "Epoch 34/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1283 - accuracy: 0.9589\n",
            "Epoch 35/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1266 - accuracy: 0.9596\n",
            "Epoch 36/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1257 - accuracy: 0.9599\n",
            "Epoch 37/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1232 - accuracy: 0.9611\n",
            "Epoch 38/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1220 - accuracy: 0.9609\n",
            "Epoch 39/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1215 - accuracy: 0.9610\n",
            "Epoch 40/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1189 - accuracy: 0.9624\n",
            "Epoch 41/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1192 - accuracy: 0.9623\n",
            "Epoch 42/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1170 - accuracy: 0.9625\n",
            "Epoch 43/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1147 - accuracy: 0.9635\n",
            "Epoch 44/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1143 - accuracy: 0.9635\n",
            "Epoch 45/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1131 - accuracy: 0.9638\n",
            "Epoch 46/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1117 - accuracy: 0.9640\n",
            "Epoch 47/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1106 - accuracy: 0.9647\n",
            "Epoch 48/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1092 - accuracy: 0.9652\n",
            "Epoch 49/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1074 - accuracy: 0.9661\n",
            "Epoch 50/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1063 - accuracy: 0.9658\n",
            "Epoch 51/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1054 - accuracy: 0.9661\n",
            "Epoch 52/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1054 - accuracy: 0.9658\n",
            "Epoch 53/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1022 - accuracy: 0.9670\n",
            "Epoch 54/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1019 - accuracy: 0.9667\n",
            "Epoch 55/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0999 - accuracy: 0.9680\n",
            "Epoch 56/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0990 - accuracy: 0.9680\n",
            "Epoch 57/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0989 - accuracy: 0.9678\n",
            "Epoch 58/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0978 - accuracy: 0.9680\n",
            "Epoch 59/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0963 - accuracy: 0.9683\n",
            "Epoch 60/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0943 - accuracy: 0.9695\n",
            "Epoch 61/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0935 - accuracy: 0.9696\n",
            "Epoch 62/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0936 - accuracy: 0.9696\n",
            "Epoch 63/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0910 - accuracy: 0.9705\n",
            "Epoch 64/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0909 - accuracy: 0.9701\n",
            "Epoch 65/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0892 - accuracy: 0.9701\n",
            "Epoch 66/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0883 - accuracy: 0.9704\n",
            "Epoch 67/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0875 - accuracy: 0.9715\n",
            "Epoch 68/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0856 - accuracy: 0.9717\n",
            "Epoch 69/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0852 - accuracy: 0.9721\n",
            "Epoch 70/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0848 - accuracy: 0.9710\n",
            "Epoch 71/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0823 - accuracy: 0.9724\n",
            "Epoch 72/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0821 - accuracy: 0.9728\n",
            "Epoch 73/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0821 - accuracy: 0.9722\n",
            "Epoch 74/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0801 - accuracy: 0.9726\n",
            "Epoch 75/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0787 - accuracy: 0.9740\n",
            "Epoch 76/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0781 - accuracy: 0.9735\n",
            "Epoch 77/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0764 - accuracy: 0.9742\n",
            "Epoch 78/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0764 - accuracy: 0.9745\n",
            "Epoch 79/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0756 - accuracy: 0.9746\n",
            "Epoch 80/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0751 - accuracy: 0.9743\n",
            "Epoch 81/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0744 - accuracy: 0.9750\n",
            "Epoch 82/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0727 - accuracy: 0.9754\n",
            "Epoch 83/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0724 - accuracy: 0.9755\n",
            "Epoch 84/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0716 - accuracy: 0.9762\n",
            "Epoch 85/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0699 - accuracy: 0.9759\n",
            "Epoch 86/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0691 - accuracy: 0.9764\n",
            "Epoch 87/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0689 - accuracy: 0.9767\n",
            "Epoch 88/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0681 - accuracy: 0.9775\n",
            "Epoch 89/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0669 - accuracy: 0.9770\n",
            "Epoch 90/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0665 - accuracy: 0.9784\n",
            "Epoch 91/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0655 - accuracy: 0.9777\n",
            "Epoch 92/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0642 - accuracy: 0.9779\n",
            "Epoch 93/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0633 - accuracy: 0.9781\n",
            "Epoch 94/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0633 - accuracy: 0.9790\n",
            "Epoch 95/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0636 - accuracy: 0.9778\n",
            "Epoch 96/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0621 - accuracy: 0.9785\n",
            "Epoch 97/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0612 - accuracy: 0.9787\n",
            "Epoch 98/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0605 - accuracy: 0.9794\n",
            "Epoch 99/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0588 - accuracy: 0.9796\n",
            "Epoch 100/100\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0597 - accuracy: 0.9794\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f8b601aa410>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezI3r2Y6RIIx"
      },
      "source": [
        "ADAM3ACC=[0.8873,0.8709,0.8814,0.8802] #accuracy of above four models keeping increasing epochs and parameters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wii2nFm3RSJs",
        "outputId": "487d4acb-5dda-42e5-d304-de6e345ff82d"
      },
      "source": [
        " print(ADAM1ACC)\n",
        " print(ADAM2ACC)\n",
        " print(ADAM3ACC)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.8873, 0.8441, 0.7746, 0.8061, 0.6327]\n",
            "[0.8873, 0.7146, 0.8298, 0.8177]\n",
            "[0.8873, 0.8709, 0.8814, 0.8802]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7C9qlz4YnPSU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "by5NesVBmHrF",
        "outputId": "a6d908b0-5d48-4470-8902-a7820eb1c548"
      },
      "source": [
        "modelADAM=keras.Sequential([\n",
        "             keras.layers.Dense(10,input_shape=(784,),activation='sigmoid') , \n",
        "             keras.layers.Dense(16,input_shape=(784,),activation='sigmoid')  ,\n",
        "             keras.layers.Dense(128,input_shape=(784,),activation='sigmoid')  ,\n",
        "             keras.layers.Dense(256,input_shape=(784,),activation='sigmoid')  \n",
        "                \n",
        "])\n",
        "modelADAM.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        "\n",
        ")\n",
        "modelADAM.fit(x_train_flattened,y_train,epochs=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 1.3263 - accuracy: 0.6003\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.5398 - accuracy: 0.8448\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.4198 - accuracy: 0.8814\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3575 - accuracy: 0.8978\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3270 - accuracy: 0.9064\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3047 - accuracy: 0.9122\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2885 - accuracy: 0.9161\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2725 - accuracy: 0.9211\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2581 - accuracy: 0.9252\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2460 - accuracy: 0.9282\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f62b4c03dd0>"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3U152JYfoJU5"
      },
      "source": [
        "y_pred=modelA134.predict(x_test_flattened)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAy7uQlCqHc_",
        "outputId": "4580aa1d-ffb4-4d3b-e547-08db6c6cea05"
      },
      "source": [
        "y_pred[0]"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3.1758656e-08, 7.4672636e-05, 3.2213566e-01, 5.0904225e-03,\n",
              "       1.6115119e-06, 1.4685149e-07, 3.8516602e-20, 9.9988818e-01,\n",
              "       1.3614469e-05, 9.8910995e-02, 5.5600271e-11, 6.0351911e-11,\n",
              "       4.5845494e-11, 4.7705336e-11, 4.5174690e-11, 3.6177162e-11,\n",
              "       4.8821829e-11, 3.9834601e-11, 4.3515563e-11, 5.1743367e-11,\n",
              "       3.7335801e-11, 4.8156035e-11, 4.2375332e-11, 3.9293992e-11,\n",
              "       5.2301809e-11, 4.7012335e-11, 4.0465992e-11, 4.7091223e-11,\n",
              "       5.2547688e-11, 4.7949998e-11, 3.0970244e-11, 4.2893324e-11,\n",
              "       4.8952745e-11, 4.5335322e-11, 3.6959647e-11, 4.5068314e-11,\n",
              "       4.2787753e-11, 4.2269584e-11, 4.6894925e-11, 4.4661077e-11,\n",
              "       4.8487186e-11, 4.4292823e-11, 4.3359552e-11, 4.7160342e-11,\n",
              "       5.7528805e-11, 4.1513498e-11, 5.0991808e-11, 4.2799018e-11,\n",
              "       4.5712250e-11, 4.9199204e-11, 5.1349418e-11, 4.9188320e-11,\n",
              "       4.0550514e-11, 3.8648949e-11, 4.3398771e-11, 4.6614643e-11,\n",
              "       4.5256101e-11, 4.7729820e-11, 3.9686181e-11, 4.8942097e-11,\n",
              "       4.5690025e-11, 5.0762696e-11, 4.7991264e-11, 3.7783041e-11,\n",
              "       3.8056360e-11, 3.6095696e-11, 4.3629170e-11, 5.0275801e-11,\n",
              "       4.7603987e-11, 4.6872752e-11, 3.5783931e-11, 5.3022996e-11,\n",
              "       4.2669663e-11, 4.8445407e-11, 5.0406582e-11, 4.4267905e-11,\n",
              "       4.2408403e-11, 3.9151328e-11, 4.8362495e-11, 4.4811425e-11,\n",
              "       4.5083187e-11, 3.7983477e-11, 4.1823454e-11, 3.9548358e-11,\n",
              "       4.3711382e-11, 3.8617269e-11, 4.8207688e-11, 4.9303291e-11,\n",
              "       5.0751275e-11, 4.9474456e-11, 4.2939249e-11, 4.2989237e-11,\n",
              "       4.1941683e-11, 4.6609314e-11, 5.3169438e-11, 3.9921920e-11,\n",
              "       5.5331770e-11, 4.8958625e-11, 3.9420481e-11, 4.3907666e-11,\n",
              "       3.9998897e-11, 4.3692130e-11, 4.1529811e-11, 4.4326036e-11,\n",
              "       4.3050938e-11, 5.7003856e-11, 3.7213663e-11, 3.5811725e-11,\n",
              "       4.6894124e-11, 4.8315848e-11, 5.0226538e-11, 4.3377999e-11,\n",
              "       3.5396391e-11, 4.5276307e-11, 5.1600071e-11, 4.6201626e-11,\n",
              "       4.4214153e-11, 4.1123317e-11, 4.9289468e-11, 4.7837637e-11,\n",
              "       4.3768027e-11, 3.8620065e-11, 5.3576834e-11, 4.6135054e-11,\n",
              "       4.3350708e-11, 4.4691501e-11, 3.6675100e-11, 5.3451802e-11,\n",
              "       4.6946030e-11, 4.8066200e-11, 4.4940846e-11, 4.8385657e-11,\n",
              "       5.1616406e-11, 3.8377527e-11, 4.3706875e-11, 5.3792585e-11,\n",
              "       4.0508697e-11, 5.2818694e-11, 5.2050021e-11, 4.6298930e-11,\n",
              "       4.8172296e-11, 5.5907279e-11, 4.5112261e-11, 4.1113120e-11,\n",
              "       4.5970536e-11, 4.9158025e-11, 4.4292396e-11, 4.3383380e-11,\n",
              "       3.4859990e-11, 4.2922218e-11, 4.6976124e-11, 4.5059633e-11,\n",
              "       4.0464680e-11, 4.2978249e-11, 5.0725240e-11, 3.7515477e-11,\n",
              "       5.0121466e-11, 5.0004105e-11, 4.1804310e-11, 4.3198171e-11,\n",
              "       5.2456060e-11, 4.8638444e-11, 3.7705936e-11, 4.3020237e-11,\n",
              "       4.1945683e-11, 3.4812944e-11, 4.4706933e-11, 4.0250459e-11,\n",
              "       4.9961684e-11, 4.0639794e-11, 4.0007060e-11, 5.3983248e-11,\n",
              "       4.0974946e-11, 5.1082343e-11, 4.4100452e-11, 4.6482974e-11,\n",
              "       3.9721854e-11, 3.9078806e-11, 5.1898250e-11, 4.8107011e-11,\n",
              "       4.3275033e-11, 4.4370615e-11, 4.0211629e-11, 4.8376980e-11,\n",
              "       4.1140577e-11, 3.9077092e-11, 5.8539278e-11, 4.1866552e-11,\n",
              "       4.1893711e-11, 5.2495789e-11, 3.9935930e-11, 4.6836736e-11,\n",
              "       5.1826928e-11, 4.1858086e-11, 5.0937764e-11, 4.2828744e-11,\n",
              "       4.3318883e-11, 4.4908795e-11, 5.2726996e-11, 4.7414423e-11,\n",
              "       4.5119922e-11, 4.9007860e-11, 4.9817619e-11, 4.4451848e-11,\n",
              "       5.4838693e-11, 3.7949300e-11, 4.0060594e-11, 5.0264969e-11,\n",
              "       4.2721698e-11, 4.5544804e-11, 5.4468947e-11, 4.3404322e-11,\n",
              "       5.3865377e-11, 4.2659407e-11, 4.1967610e-11, 4.3734148e-11,\n",
              "       3.3871149e-11, 5.0382355e-11, 5.1951242e-11, 4.5527519e-11,\n",
              "       3.9475707e-11, 4.2057111e-11, 4.6274564e-11, 5.1210629e-11,\n",
              "       4.8829555e-11, 4.0904044e-11, 5.1511764e-11, 4.0173451e-11,\n",
              "       3.7969645e-11, 4.3342434e-11, 3.6279361e-11, 4.4912223e-11,\n",
              "       3.5299978e-11, 4.3995349e-11, 4.4336347e-11, 4.2352063e-11,\n",
              "       4.4311246e-11, 5.1091402e-11, 4.2741911e-11, 4.3735144e-11,\n",
              "       5.9330173e-11, 3.5409630e-11, 5.0087726e-11, 3.7741265e-11,\n",
              "       4.0714741e-11, 3.8563090e-11, 4.3008180e-11, 3.7148350e-11,\n",
              "       4.9386113e-11, 5.9292848e-11, 5.5677105e-11, 5.1242673e-11,\n",
              "       5.7123157e-11, 3.9057122e-11, 5.0888238e-11, 3.9374941e-11,\n",
              "       3.9730948e-11, 5.5075802e-11, 4.6938241e-11, 4.7030185e-11,\n",
              "       3.8509949e-11, 4.1405709e-11, 4.7131209e-11, 3.9693152e-11,\n",
              "       5.0217917e-11, 4.1526636e-11, 5.1812599e-11, 5.0040846e-11,\n",
              "       4.9280066e-11, 4.4702925e-11, 5.1692175e-11, 4.9501920e-11,\n",
              "       4.3941344e-11, 4.5277081e-11, 4.3667715e-11, 4.3600883e-11,\n",
              "       4.0368396e-11, 4.5405072e-11, 4.4098853e-11, 4.3976555e-11,\n",
              "       5.7188233e-11, 4.7465087e-11, 4.6735629e-11, 3.4862713e-11,\n",
              "       4.5403861e-11, 5.1502039e-11, 4.6930720e-11, 4.8032842e-11,\n",
              "       4.1568089e-11, 4.1649593e-11, 4.6710413e-11, 4.3525523e-11,\n",
              "       4.3720048e-11, 4.3739654e-11, 5.4707166e-11, 4.2790119e-11,\n",
              "       4.3552759e-11, 4.8444113e-11, 4.3805112e-11, 4.3542874e-11,\n",
              "       4.4642172e-11, 4.7088528e-11, 4.1467121e-11, 4.1531948e-11,\n",
              "       4.9117349e-11, 4.0763851e-11, 5.3689466e-11, 4.1764304e-11,\n",
              "       4.1968890e-11, 3.7960378e-11, 5.0543774e-11, 4.4747026e-11,\n",
              "       3.8912797e-11, 6.6273619e-11, 3.6315017e-11, 4.0957439e-11,\n",
              "       3.7269920e-11, 4.3615524e-11, 4.8519661e-11, 4.4296792e-11,\n",
              "       4.5883113e-11, 3.8461137e-11, 5.2882098e-11, 4.6436365e-11,\n",
              "       4.9735115e-11, 4.3163008e-11, 4.2553277e-11, 3.6051519e-11,\n",
              "       5.2613476e-11, 4.4465584e-11, 4.6415996e-11, 5.0578101e-11,\n",
              "       5.7585672e-11, 5.4446714e-11, 4.8290049e-11, 3.5121468e-11,\n",
              "       4.8090223e-11, 4.8950687e-11, 4.3296994e-11, 5.6469277e-11,\n",
              "       5.3359917e-11, 4.6985537e-11, 4.1201279e-11, 5.4702784e-11,\n",
              "       4.6728232e-11, 4.5876108e-11, 4.7133280e-11, 3.9919252e-11,\n",
              "       3.9534025e-11, 3.9978153e-11, 5.3891482e-11, 4.5984216e-11,\n",
              "       4.5965884e-11, 4.1542800e-11, 4.3712218e-11, 4.9539137e-11,\n",
              "       4.1370345e-11, 3.8032633e-11, 6.2668856e-11, 4.8814286e-11,\n",
              "       4.3972031e-11, 3.9289644e-11, 3.6465081e-11, 4.6852910e-11,\n",
              "       3.9666128e-11, 3.2945716e-11, 5.1945493e-11, 3.8984409e-11,\n",
              "       4.3913862e-11, 4.9210844e-11, 5.0571450e-11, 6.3526087e-11,\n",
              "       3.2095299e-11, 3.8304103e-11, 3.5581458e-11, 5.2449354e-11,\n",
              "       5.7643029e-11, 4.2424342e-11, 4.0653751e-11, 3.9208869e-11,\n",
              "       5.0515529e-11, 3.9433113e-11, 4.2635166e-11, 4.2925656e-11,\n",
              "       4.2488638e-11, 4.6586290e-11, 4.9574962e-11, 3.8306951e-11,\n",
              "       4.6171941e-11, 3.9938369e-11, 4.6986158e-11, 5.6412760e-11,\n",
              "       4.3642319e-11, 4.8283145e-11, 4.6258240e-11, 4.4546311e-11,\n",
              "       4.8535301e-11, 4.4807495e-11, 6.2931181e-11, 5.0210443e-11,\n",
              "       4.7426452e-11, 5.7172309e-11, 4.7189405e-11, 5.3838468e-11,\n",
              "       4.1738023e-11, 4.1751078e-11, 4.8317132e-11, 5.5823100e-11,\n",
              "       4.1242631e-11, 5.5090089e-11, 4.0955021e-11, 5.1474543e-11,\n",
              "       4.4667558e-11, 6.1464944e-11, 6.0242880e-11, 4.3472507e-11,\n",
              "       4.0813953e-11, 5.4308669e-11, 4.0894215e-11, 5.7148654e-11,\n",
              "       5.4578238e-11, 4.0392040e-11, 3.5889881e-11, 4.7842019e-11,\n",
              "       4.9056773e-11, 4.7678500e-11, 5.4727306e-11, 4.7216873e-11,\n",
              "       4.2560011e-11, 3.9518423e-11, 5.4886411e-11, 4.0890392e-11,\n",
              "       5.0197999e-11, 4.1052467e-11, 3.4358506e-11, 4.1339189e-11,\n",
              "       3.6813445e-11, 4.0667168e-11, 4.3574696e-11, 4.5483950e-11,\n",
              "       3.7132408e-11, 5.2291727e-11, 4.9335463e-11, 5.1360291e-11,\n",
              "       4.7999320e-11, 4.0286559e-11, 4.8804790e-11, 4.9547928e-11,\n",
              "       3.9526184e-11, 4.0687644e-11, 4.0782513e-11, 4.3726311e-11,\n",
              "       5.5864320e-11, 4.1294742e-11, 5.8336752e-11, 3.5468302e-11,\n",
              "       4.0585646e-11, 5.2836829e-11, 4.7796503e-11, 5.2258923e-11,\n",
              "       5.7745263e-11, 5.0033113e-11, 4.2683499e-11, 4.2321622e-11,\n",
              "       4.8630100e-11, 4.7198592e-11, 4.9882747e-11, 5.1397529e-11,\n",
              "       3.6452910e-11, 5.6475744e-11, 3.4569510e-11, 3.8207201e-11,\n",
              "       3.9986313e-11, 4.2332030e-11, 4.0984403e-11, 4.8269236e-11,\n",
              "       3.8931726e-11, 4.6084133e-11, 3.7799978e-11, 4.6296987e-11,\n",
              "       3.6596490e-11, 4.6112621e-11, 4.2685452e-11, 5.2336531e-11,\n",
              "       4.8811306e-11, 5.1007667e-11, 5.5523971e-11, 4.5236596e-11,\n",
              "       5.6342542e-11, 3.8296141e-11, 4.0331224e-11, 4.4608719e-11,\n",
              "       5.7231012e-11, 3.9361053e-11, 4.4863595e-11, 4.2610453e-11,\n",
              "       4.2370968e-11, 4.2007970e-11, 4.2745175e-11, 3.6239733e-11,\n",
              "       4.2055668e-11, 3.8591418e-11, 3.6298672e-11, 4.2094352e-11,\n",
              "       4.9481440e-11, 5.0456701e-11, 5.1204475e-11, 4.4235577e-11,\n",
              "       5.3991280e-11, 4.0294858e-11, 4.3280317e-11, 4.8760874e-11,\n",
              "       5.1748997e-11, 4.1883719e-11, 4.6523514e-11, 3.6721785e-11,\n",
              "       4.0056541e-11, 3.9965573e-11, 3.7869832e-11, 5.5010291e-11,\n",
              "       4.3805112e-11, 4.6432469e-11, 5.2307388e-11, 5.1514813e-11,\n",
              "       4.4107603e-11, 4.7780554e-11, 4.2062170e-11, 4.5625843e-11,\n",
              "       3.8509803e-11, 4.3224462e-11, 4.8313725e-11, 5.1360194e-11,\n",
              "       4.2481105e-11, 5.3641258e-11, 4.6411659e-11, 3.7400878e-11,\n",
              "       4.9698703e-11, 4.3281469e-11, 4.1239252e-11, 4.1192553e-11,\n",
              "       4.3758257e-11, 4.6574741e-11, 4.1286946e-11, 4.9932437e-11,\n",
              "       3.8933062e-11, 3.8856800e-11, 4.4890983e-11, 5.1874997e-11,\n",
              "       4.9477377e-11, 4.7993276e-11, 4.0399822e-11, 3.5466814e-11,\n",
              "       3.9152445e-11, 4.4624124e-11, 3.8370498e-11, 4.2978249e-11,\n",
              "       6.6249353e-11, 5.2445451e-11, 4.0372167e-11, 4.0442687e-11,\n",
              "       4.7663953e-11, 4.8723685e-11, 5.5209799e-11, 4.4547758e-11,\n",
              "       4.4572235e-11, 4.7630507e-11, 4.6438752e-11, 4.2546293e-11,\n",
              "       5.6909560e-11, 4.4462701e-11, 4.8569475e-11, 3.2723827e-11,\n",
              "       4.8964038e-11, 4.0683845e-11, 4.7174924e-11, 3.6556657e-11,\n",
              "       4.4280826e-11, 4.6687786e-11, 4.4510728e-11, 4.4449729e-11,\n",
              "       5.4385579e-11, 5.4116545e-11, 4.5474496e-11, 4.8612198e-11,\n",
              "       4.0648710e-11, 5.0469306e-11, 3.6824262e-11, 4.7131032e-11,\n",
              "       5.0137717e-11, 4.2013417e-11, 4.2589161e-11, 5.2966187e-11,\n",
              "       5.8258467e-11, 5.7128160e-11, 4.2751126e-11, 4.6152308e-11,\n",
              "       4.3512076e-11, 4.1127626e-11, 4.7091938e-11, 4.2648912e-11,\n",
              "       5.6833201e-11, 3.9332159e-11, 4.0646382e-11, 4.5873135e-11,\n",
              "       4.2602814e-11, 4.5705280e-11, 3.8822501e-11, 4.5415203e-11,\n",
              "       4.7094096e-11, 4.8880715e-11, 4.4371878e-11, 5.5996281e-11,\n",
              "       4.6806999e-11, 5.8263026e-11, 5.8993907e-11, 4.4735337e-11,\n",
              "       4.8024227e-11, 4.3609949e-11, 4.2079982e-11, 5.2832294e-11,\n",
              "       3.3976162e-11, 5.0926787e-11, 4.0157898e-11, 4.9933294e-11,\n",
              "       4.6869803e-11, 4.2332998e-11, 4.1305927e-11, 4.2595014e-11,\n",
              "       4.2473487e-11, 3.6349944e-11, 4.6063223e-11, 4.4104408e-11,\n",
              "       4.5645859e-11, 4.6011851e-11, 3.7092981e-11, 6.2599689e-11,\n",
              "       5.4681398e-11, 5.4490766e-11, 3.7824861e-11, 4.6536736e-11,\n",
              "       5.4498978e-11, 4.7485377e-11, 4.7221282e-11, 4.6792271e-11,\n",
              "       3.9895431e-11, 4.5417545e-11, 5.0184405e-11, 3.5298504e-11,\n",
              "       5.6209339e-11, 4.3327893e-11, 5.1861047e-11, 5.1068216e-11,\n",
              "       4.4525838e-11, 5.1500568e-11, 6.2537274e-11, 4.9199953e-11,\n",
              "       5.2843181e-11, 3.7483003e-11, 3.2383693e-11, 4.6294781e-11,\n",
              "       4.7557340e-11, 4.4428454e-11, 4.6532833e-11, 3.9044611e-11,\n",
              "       4.0903652e-11, 5.3831280e-11, 5.3104063e-11, 4.4021453e-11,\n",
              "       4.0315612e-11, 4.8434593e-11, 4.1957042e-11, 4.9219292e-11,\n",
              "       4.4124770e-11, 4.2238876e-11, 5.4240654e-11, 4.5674863e-11,\n",
              "       5.8141804e-11, 4.0508076e-11, 4.8395624e-11, 4.3725558e-11,\n",
              "       3.8109023e-11, 5.2486581e-11, 4.0587041e-11, 4.5820320e-11,\n",
              "       4.8042278e-11, 4.9004592e-11, 4.5071322e-11, 5.0454484e-11,\n",
              "       4.4556511e-11, 4.8865797e-11, 4.4117363e-11, 4.6419094e-11,\n",
              "       4.6343106e-11, 4.0407143e-11, 5.1339325e-11, 4.7886660e-11,\n",
              "       4.3044041e-11, 4.7310263e-11, 4.9298302e-11, 3.6256328e-11,\n",
              "       3.5779359e-11, 3.3814989e-11, 5.1830092e-11, 4.8183232e-11,\n",
              "       4.0264889e-11, 4.0470932e-11, 3.8318571e-11, 4.2074365e-11,\n",
              "       4.2938101e-11, 6.6069789e-11, 5.2880787e-11, 4.5106325e-11,\n",
              "       5.4950624e-11, 5.9475314e-11, 3.8535005e-11, 3.7336446e-11,\n",
              "       4.4816300e-11, 5.5632516e-11, 4.3930699e-11, 4.1688378e-11,\n",
              "       4.7567682e-11, 4.3720555e-11, 5.2190492e-11, 4.1199155e-11,\n",
              "       4.5579915e-11, 4.0349536e-11, 4.0838638e-11, 4.0511788e-11,\n",
              "       4.3720638e-11, 4.8201890e-11, 4.5223741e-11, 4.5254977e-11,\n",
              "       4.7513368e-11, 4.1743276e-11, 4.3985114e-11, 4.9942155e-11,\n",
              "       4.0863805e-11, 4.7786657e-11, 5.3785403e-11, 4.6981950e-11,\n",
              "       4.4897319e-11, 4.1772266e-11, 4.5598258e-11, 4.3559486e-11,\n",
              "       4.6648355e-11, 4.5644294e-11, 4.3185816e-11, 4.9874282e-11,\n",
              "       5.2811637e-11, 4.5948179e-11, 4.1535039e-11, 4.3731397e-11,\n",
              "       4.6965376e-11, 4.7247227e-11, 4.2870259e-11, 4.3983771e-11,\n",
              "       4.7418309e-11, 4.1883965e-11, 5.6945403e-11, 4.3326901e-11,\n",
              "       4.8670647e-11, 4.3416489e-11, 4.9343741e-11, 4.8999447e-11,\n",
              "       3.8771146e-11, 4.4139917e-11, 4.0507923e-11, 4.8613405e-11,\n",
              "       4.6629849e-11, 4.7192198e-11, 4.0806868e-11, 4.4022883e-11,\n",
              "       4.1390467e-11, 4.6870698e-11, 4.1370501e-11, 4.7681230e-11,\n",
              "       4.7113414e-11, 4.3563559e-11, 5.6503864e-11, 4.7580297e-11,\n",
              "       4.4422351e-11, 5.0263241e-11, 4.8480067e-11, 4.3402993e-11,\n",
              "       4.1916810e-11, 4.4231785e-11, 4.9512120e-11, 6.3321411e-11,\n",
              "       4.3474828e-11, 4.1103003e-11, 3.6621976e-11, 4.0764399e-11,\n",
              "       4.3200311e-11, 4.1080903e-11, 4.2816899e-11, 4.4831173e-11,\n",
              "       4.4638338e-11, 4.7555619e-11, 5.3652409e-11, 4.2074122e-11,\n",
              "       4.3649397e-11, 4.4903487e-11, 4.3229243e-11, 3.9401309e-11,\n",
              "       4.1254986e-11, 4.4828436e-11, 4.4899980e-11, 4.2353600e-11,\n",
              "       5.3816802e-11, 4.8001610e-11, 3.9839080e-11, 4.3047819e-11,\n",
              "       3.8785201e-11, 5.3069837e-11, 4.7126535e-11, 4.8164670e-11,\n",
              "       4.0425416e-11, 4.6449122e-11, 5.5288302e-11, 4.6438752e-11,\n",
              "       3.9308983e-11, 4.6638571e-11, 4.4148005e-11, 5.0413406e-11,\n",
              "       4.3132716e-11, 4.4577082e-11, 3.9302759e-11, 5.3805821e-11,\n",
              "       4.2427419e-11, 5.0721566e-11, 4.2536964e-11, 3.9968012e-11,\n",
              "       3.4777105e-11, 5.1577145e-11, 4.0272879e-11, 3.9264845e-11,\n",
              "       3.3350919e-11, 4.1332646e-11, 5.9404003e-11, 4.2220675e-11,\n",
              "       5.5812240e-11, 4.7749395e-11, 4.4321807e-11, 4.8472670e-11,\n",
              "       4.5222364e-11, 5.4131928e-11, 4.8534746e-11, 3.9407623e-11,\n",
              "       4.7962075e-11, 4.5182792e-11, 4.0773881e-11, 4.2016942e-11,\n",
              "       5.2367690e-11, 3.9305083e-11, 4.6235391e-11, 3.6632593e-11,\n",
              "       5.1492414e-11, 3.9117362e-11, 4.3052908e-11, 4.5420577e-11,\n",
              "       5.0296423e-11, 4.0496412e-11, 4.5722627e-11, 3.6873671e-11,\n",
              "       5.1985933e-11, 4.6340893e-11, 3.6389069e-11, 3.8069502e-11,\n",
              "       3.7735436e-11, 4.1461976e-11, 4.9909202e-11, 5.2473671e-11,\n",
              "       4.9409008e-11, 3.6678736e-11, 4.7853426e-11, 5.3495250e-11,\n",
              "       5.6198400e-11, 4.2607938e-11, 4.6423875e-11, 4.6621491e-11,\n",
              "       5.1641517e-11, 4.4418115e-11, 4.0798930e-11, 4.5583304e-11,\n",
              "       4.8737812e-11, 4.1597514e-11, 3.5727601e-11, 3.5300519e-11,\n",
              "       4.5835443e-11, 4.6900834e-11, 4.2233723e-11, 4.2707681e-11,\n",
              "       4.8605710e-11, 4.1919926e-11, 6.2701504e-11, 4.9450680e-11,\n",
              "       5.1231921e-11, 4.4268918e-11, 3.7795222e-11, 4.3634908e-11,\n",
              "       3.7930919e-11, 4.5450910e-11, 4.1875971e-11, 3.7677757e-11,\n",
              "       4.7979731e-11, 4.2499743e-11, 5.1596231e-11, 5.0521119e-11,\n",
              "       4.7146405e-11, 4.8337601e-11, 4.9039280e-11, 4.6696244e-11,\n",
              "       3.8049244e-11, 4.2220508e-11, 4.1601399e-11, 4.3199406e-11,\n",
              "       4.5414773e-11, 4.2313791e-11, 5.2000709e-11, 4.1220301e-11,\n",
              "       4.8348939e-11, 4.4396094e-11, 4.2119648e-11, 3.9326303e-11,\n",
              "       3.9923898e-11, 4.2588270e-11, 4.4235074e-11, 4.4693461e-11,\n",
              "       5.1192946e-11, 3.6955272e-11, 4.8424618e-11, 5.3724088e-11,\n",
              "       4.4219045e-11, 4.1180222e-11, 5.2486182e-11, 4.0968930e-11,\n",
              "       5.1351864e-11, 5.1096082e-11, 4.6041698e-11, 5.5541446e-11,\n",
              "       3.6419274e-11, 4.9934626e-11, 4.4023802e-11, 4.1598464e-11,\n",
              "       4.5271467e-11, 4.0226277e-11, 3.9560955e-11, 4.6518279e-11,\n",
              "       4.9962447e-11, 5.3273278e-11, 5.1407333e-11, 4.7510648e-11,\n",
              "       4.8620451e-11, 5.9028012e-11, 4.6736257e-11, 4.2310318e-11,\n",
              "       5.0000198e-11, 4.2546133e-11, 4.7324700e-11, 5.1310258e-11,\n",
              "       4.6619712e-11, 3.7249454e-11, 4.6206563e-11, 4.3013183e-11,\n",
              "       4.8418797e-11, 4.3600717e-11, 4.4704886e-11, 4.8232057e-11,\n",
              "       4.1722500e-11, 4.0909817e-11, 4.1312388e-11, 4.9485877e-11,\n",
              "       4.8601536e-11, 4.5943100e-11, 4.3550684e-11, 4.1634897e-11,\n",
              "       5.5630608e-11, 5.5495063e-11, 4.1801201e-11, 5.0614093e-11,\n",
              "       3.7363872e-11, 4.6875611e-11, 4.1710885e-11, 4.6246857e-11,\n",
              "       4.4586092e-11, 6.0762874e-11, 4.9966451e-11, 4.9179973e-11,\n",
              "       3.9800878e-11, 4.2332918e-11, 3.9674455e-11, 4.0054016e-11,\n",
              "       3.8729533e-11, 4.4422521e-11, 4.4510982e-11, 5.6698733e-11,\n",
              "       4.0463133e-11, 4.4292230e-11, 4.8282770e-11, 4.8495971e-11],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "id": "M7HjuHtnqsPW",
        "outputId": "7361cb1b-0736-45c6-fe12-322f5930de38"
      },
      "source": [
        "plt.matshow(x_test[0])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f8b1f2ed350>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 288x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ulkxcgg1q-YK",
        "outputId": "82236186-0651-4fa0-c112-d1ee82a71f1d"
      },
      "source": [
        "np.argmax(y_pred[0])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NATsc-nkraLv"
      },
      "source": [
        "y_pred_f=[np.argmax(i) for i in y_pred]"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nTp3myT4vc9b",
        "outputId": "6504a649-28bf-4c4b-8474-94364f1be555"
      },
      "source": [
        "y_pred_f[:5]"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[7, 2, 1, 0, 4]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvXYMybNveoV",
        "outputId": "7dbfab0c-6b1e-4103-ac87-1ac873a39d46"
      },
      "source": [
        "from keras import metrics \n",
        "from sklearn.metrics import *\n",
        "adam_acc=accuracy_score(y_pred_f,y_test)\n",
        "print(adam_acc)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9393\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-taL4DvxzY2"
      },
      "source": [
        "SGd --Gradient descent (with momentum) optimizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8CRVKriwT2U",
        "outputId": "b0638baa-a07d-49cd-876c-44cb8ee7abf3"
      },
      "source": [
        "modelSGD=keras.Sequential([\n",
        "             keras.layers.Dense(10,input_shape=(784,),activation='sigmoid') , \n",
        "             keras.layers.Dense(16,input_shape=(784,),activation='sigmoid')  ,\n",
        "             keras.layers.Dense(128,input_shape=(784,),activation='sigmoid')  ,\n",
        "             keras.layers.Dense(256,input_shape=(784,),activation='sigmoid')  \n",
        "                \n",
        "])\n",
        "opt = tf.keras.optimizers.SGD(learning_rate=0.1)\n",
        "modelSGD.compile(\n",
        "    optimizer=opt,\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        "\n",
        ")\n",
        "modelSGD.fit(x_train_flattened,y_train,epochs=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 2.2242 - accuracy: 0.1616\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.1511 - accuracy: 0.5801\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.7272 - accuracy: 0.7789\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.5606 - accuracy: 0.8455\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.4573 - accuracy: 0.8747\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.3976 - accuracy: 0.8907\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.3581 - accuracy: 0.8997\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.3275 - accuracy: 0.9083\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.3035 - accuracy: 0.9138\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.2851 - accuracy: 0.9186\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f62b0b51b50>"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwacIcFsyMtM"
      },
      "source": [
        "y_pred_sgd=modelSGD.predict(x_test_flattened)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9Kq2Hijy05q"
      },
      "source": [
        "y_pred_SGDf=[np.argmax(i) for i in y_pred_sgd]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svBt1PHvy6_B",
        "outputId": "b07caeb2-e82a-49a0-f427-76faa6ae0edb"
      },
      "source": [
        "ACC_sgd=accuracy_score(y_pred_SGDf,y_test)\n",
        "print(ACC_sgd)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.8113\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYxoXI2UzFlH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}